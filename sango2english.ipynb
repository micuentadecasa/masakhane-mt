{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sango2english.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMAhSCmlUmvbeGon70s7RHu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micuentadecasa/masakhane-mt/blob/Sango/sango2english.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUyKNxsXZQIU",
        "outputId": "8c182701-9e81-4630-ef28-e23573f0e808"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fzls7kSZYaN"
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"sg\"\n",
        "target_language = \"en\" \n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# # This will save it to a folder in our gdrive instead!\n",
        "# !mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
        "# os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)\n",
        "\n",
        "# This will save it to a folder in our gdrive instead! \n",
        "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
        "g_drive_path = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)\n",
        "os.environ[\"gdrive_path\"] = g_drive_path\n",
        "models_path = '%s/models/%s%s_transformer'% (g_drive_path, source_language, target_language)\n",
        "# model temporary directory for training\n",
        "model_temp_dir = \"/content/drive/My Drive/masakhane/model-temp\"\n",
        "# model permanent storage on the drive\n",
        "!mkdir -p \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYPNz94hZzTm",
        "outputId": "75efd9bb-7a58-4d2e-cd03-55735a368d97"
      },
      "source": [
        "!echo $gdrive_path"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masakhane/sg-en-baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb5k0AlHafXR",
        "outputId": "e007bb99-4860-4491-8a1f-2686170c377f"
      },
      "source": [
        "# Install opus-tools\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opustools-pkg\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/9f/e829a0cceccc603450cd18e1ff80807b6237a88d9a8df2c0bb320796e900/opustools_pkg-0.0.52-py3-none-any.whl (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.2MB/s \n",
            "\u001b[?25hInstalling collected packages: opustools-pkg\n",
            "Successfully installed opustools-pkg-0.0.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf0NFk8yaiwE",
        "outputId": "854ea1c4-ed6a-4525-a19b-3330084caa0c"
      },
      "source": [
        "# Downloading our corpus\n",
        "! opus_read -d JW300 -s $src -t $tgt -wm moses -w jw300.$src jw300.$tgt -q\n",
        "# extract the corpus file\n",
        "! gunzip JW300_latest_xml_$tgt-$src.xml.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/JW300/latest/xml/en-sg.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "   3 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en-sg.xml.gz\n",
            " 263 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en.zip\n",
            "  33 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/sg.zip\n",
            "\n",
            " 299 MB Total size\n",
            "./JW300_latest_xml_en-sg.xml.gz ... 100% of 3 MB\n",
            "./JW300_latest_xml_en.zip ... 100% of 263 MB\n",
            "./JW300_latest_xml_sg.zip ... 100% of 33 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "FkzjiCk5arL7",
        "outputId": "8265105e-5171-4104-b632-c90094abf486"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TMX file to dataframe\n",
        "source_file = 'jw300.' + source_language\n",
        "target_file = 'jw300.' + target_language\n",
        "\n",
        "source = []\n",
        "target = []\n",
        "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
        "with open(source_file) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        # Skip sentences that are contained in the test set.\n",
        "        # if line.strip() not in en_test_sents:\n",
        "        source.append(line.strip())\n",
        "        # else:\n",
        "        #     skip_lines.append(i)             \n",
        "with open(target_file) as f:\n",
        "    for j, line in enumerate(f):\n",
        "        # Only add to corpus if corresponding source was not skipped.\n",
        "        # if j not in skip_lines:\n",
        "        target.append(line.strip())\n",
        "    \n",
        "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
        "    \n",
        "#df = pd.DataFrame(zip(source_file, target_file), columns=['source_sentence', 'target_sentence'])\n",
        "# if you get TypeError: data argument can't be an iterator is because of your zip version run this below\n",
        "df1= pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])\n",
        "print(df1.shape)\n",
        "df1.head(8)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data and skipped 0/289194 lines since contained in test set.\n",
            "(289195, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>The Bible’s Viewpoint</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>“ I yeke ti sese so pëpe ” : Tongana nyen ?</td>\n",
              "      <td>“ No Part of the World ” — What Does It Mean ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NA SIÈCLE osio ti ngoi ti e , azo saki mingi s...</td>\n",
              "      <td>IN THE fourth century C.E . , thousands of pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Na salango tongaso ala ga a - anachorète .</td>\n",
              "      <td>They became known as anchorites , from the Gre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tënë so alondo na tënë anakorêtès na Grec , so...</td>\n",
              "      <td>One historian describes them as holding themse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Mbeni wasungo - mbaï atene so ala ke songo kue...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Na zingo tele ti ala yamba na tanga ti azo , a...</td>\n",
              "      <td>Anchorites thought that by withdrawing from hu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>A yeke tâ tënë so Bible awa aChrétien ti ‘ bat...</td>\n",
              "      <td>The Bible does admonish Christians to keep “ w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     source_sentence                                    target_sentence\n",
              "0                                                                                 The Bible’s Viewpoint\n",
              "1        “ I yeke ti sese so pëpe ” : Tongana nyen ?     “ No Part of the World ” — What Does It Mean ?\n",
              "2  NA SIÈCLE osio ti ngoi ti e , azo saki mingi s...  IN THE fourth century C.E . , thousands of pro...\n",
              "3         Na salango tongaso ala ga a - anachorète .  They became known as anchorites , from the Gre...\n",
              "4  Tënë so alondo na tënë anakorêtès na Grec , so...  One historian describes them as holding themse...\n",
              "5  Mbeni wasungo - mbaï atene so ala ke songo kue...                                                   \n",
              "6  Na zingo tele ti ala yamba na tanga ti azo , a...  Anchorites thought that by withdrawing from hu...\n",
              "7  A yeke tâ tënë so Bible awa aChrétien ti ‘ bat...  The Bible does admonish Christians to keep “ w..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGnjF2CRbTY-",
        "outputId": "fea44914-a5d7-4248-a69b-dc18ef6f8212"
      },
      "source": [
        "# Downloading our corpus\n",
        "! opus_read -d Tatoeba -s $src -t $tgt -wm moses -w tatoeba.$src tatoeba.$tgt -q\n",
        "\n",
        "# extract the corpus file\n",
        "! gunzip Tatoeba_latest_xml_$tgt-$src.xml.gz"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/Tatoeba/latest/xml/en-sg.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "   4 KB https://object.pouta.csc.fi/OPUS-Tatoeba/v2020-11-09/xml/en-sg.xml.gz\n",
            " 109 MB https://object.pouta.csc.fi/OPUS-Tatoeba/v2020-11-09/xml/en.zip\n",
            "   8 KB https://object.pouta.csc.fi/OPUS-Tatoeba/v2020-11-09/xml/sg.zip\n",
            "\n",
            " 109 MB Total size\n",
            "./Tatoeba_latest_xml_en-sg.xml.gz ... 100% of 4 KB\n",
            "./Tatoeba_latest_xml_en.zip ... 100% of 109 MB\n",
            "./Tatoeba_latest_xml_sg.zip ... 100% of 8 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "NKLalwP_bvzn",
        "outputId": "7c7183b8-21b3-482d-ec50-36ab837de740"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TMX file to dataframe\n",
        "source_file = 'tatoeba.' + source_language\n",
        "target_file = 'tatoeba.' + target_language\n",
        "\n",
        "source = []\n",
        "target = []\n",
        "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
        "with open(source_file) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        # Skip sentences that are contained in the test set.\n",
        "        # if line.strip() not in en_test_sents:\n",
        "        source.append(line.strip())\n",
        "        # else:\n",
        "        #     skip_lines.append(i)             \n",
        "with open(target_file) as f:\n",
        "    for j, line in enumerate(f):\n",
        "        # Only add to corpus if corresponding source was not skipped.\n",
        "        # if j not in skip_lines:\n",
        "        target.append(line.strip())\n",
        "    \n",
        "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
        "    \n",
        "#df = pd.DataFrame(zip(source_file, target_file), columns=['source_sentence', 'target_sentence'])\n",
        "# if you get TypeError: data argument can't be an iterator is because of your zip version run this below\n",
        "df2= pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])\n",
        "print(df2.shape)\n",
        "df2.head(3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data and skipped 0/10 lines since contained in test set.\n",
            "(11, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ïrï tî mo nyen ?</td>\n",
              "      <td>What is your name ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Balaô !</td>\n",
              "      <td>Hello !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Singîla !</td>\n",
              "      <td>Thank you !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    source_sentence      target_sentence\n",
              "0  Ïrï tî mo nyen ?  What is your name ?\n",
              "1           Balaô !              Hello !\n",
              "2         Singîla !          Thank you !"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB15zJARcls1",
        "outputId": "6ac98a32-a538-4b50-dce6-6759a3fd371e"
      },
      "source": [
        "# Downloading our corpus\n",
        "! opus_read -d wikimedia -s $src -t $tgt -wm moses -w wikimedia.$src wikimedia.$tgt -q\n",
        "\n",
        "# extract the corpus file\n",
        "! gunzip wikimedia_latest_xml_$tgt-$src.xml.gz"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/wikimedia/latest/xml/en-sg.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "\n",
            "   0 KB Total size\n",
            "No alignment file \"/proj/nlpl/data/OPUS/wikimedia/latest/xml/en-sg.xml.gz\" or \"./wikimedia_latest_xml_en-sg.xml.gz\" found\n",
            "gzip: wikimedia_latest_xml_en-sg.xml.gz: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "fKzXpUp-c7QB",
        "outputId": "b377a7ba-4a09-4229-a7d8-77275c48e9a8"
      },
      "source": [
        "# TMX file to dataframe\n",
        "source_file = 'wikimedia.' + source_language\n",
        "target_file = 'wikimedia.' + target_language\n",
        "\n",
        "source = []\n",
        "target = []\n",
        "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
        "with open(source_file) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        # Skip sentences that are contained in the test set.\n",
        "        # if line.strip() not in en_test_sents:\n",
        "        source.append(line.strip())\n",
        "        # else:\n",
        "        #     skip_lines.append(i)             \n",
        "with open(target_file) as f:\n",
        "    for j, line in enumerate(f):\n",
        "        # Only add to corpus if corresponding source was not skipped.\n",
        "        # if j not in skip_lines:\n",
        "        target.append(line.strip())\n",
        "    \n",
        "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
        "    \n",
        "#df = pd.DataFrame(zip(source_file, target_file), columns=['source_sentence', 'target_sentence'])\n",
        "# if you get TypeError: data argument can't be an iterator is because of your zip version run this below\n",
        "df3= pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])\n",
        "print(df3.shape)\n",
        "df3.head(3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data and skipped 0/10 lines since contained in test set.\n",
            "(0, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [source_sentence, target_sentence]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RymAMn6dTjW",
        "outputId": "0d62d168-19a3-4ea5-93e4-8187b34e356d"
      },
      "source": [
        "# Downloading our corpus\n",
        "! opus_read -d Tanzil -s $src -t $tgt -wm moses -w tanzil.$src tanzil.$tgt -q\n",
        "\n",
        "# extract the corpus file\n",
        "! gunzip Tanzil_latest_xml_$tgt-$src.xml.gz"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/Tanzil/latest/xml/en-sg.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "\n",
            "   0 KB Total size\n",
            "No alignment file \"/proj/nlpl/data/OPUS/Tanzil/latest/xml/en-sg.xml.gz\" or \"./Tanzil_latest_xml_en-sg.xml.gz\" found\n",
            "gzip: Tanzil_latest_xml_en-sg.xml.gz: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "Fr_YIZx9dYE3",
        "outputId": "674b2328-8294-4ede-eb64-140ac17c603c"
      },
      "source": [
        "# TMX file to dataframe\n",
        "source_file = 'tanzil.' + source_language\n",
        "target_file = 'tanzil.' + target_language\n",
        "\n",
        "source = []\n",
        "target = []\n",
        "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
        "with open(source_file) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        # Skip sentences that are contained in the test set.\n",
        "        # if line.strip() not in en_test_sents:\n",
        "        source.append(line.strip())\n",
        "        # else:\n",
        "        #     skip_lines.append(i)             \n",
        "with open(target_file) as f:\n",
        "    for j, line in enumerate(f):\n",
        "        # Only add to corpus if corresponding source was not skipped.\n",
        "        # if j not in skip_lines:\n",
        "        target.append(line.strip())\n",
        "    \n",
        "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
        "    \n",
        "#df = pd.DataFrame(zip(source_file, target_file), columns=['source_sentence', 'target_sentence'])\n",
        "# if you get TypeError: data argument can't be an iterator is because of your zip version run this below\n",
        "df4= pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])\n",
        "print(df4.shape)\n",
        "df4.head(3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data and skipped 0/10 lines since contained in test set.\n",
            "(0, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [source_sentence, target_sentence]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sknaeMfdcvl"
      },
      "source": [
        "# in our case for sango the df3 and df4 are empty\n",
        "List = [df1, df2, df3, df4]  # List of your dataframes\n",
        "df = pd.concat(List)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4kYH2ladl0l",
        "outputId": "1a75ff39-703f-4a36-f35c-d6d2ea72e2a9"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(289206, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQeU1nK2dowE"
      },
      "source": [
        "# drop duplicate translations\n",
        "df_pp = df.drop_duplicates()\n",
        "\n",
        "# drop conflicting translations\n",
        "# (this is optional and something that you might want to comment out \n",
        "# depending on the size of your corpus)\n",
        "#df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
        "#df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
        "\n",
        "# Shuffle the data to remove bias in dev set selection.\n",
        "df_pp = df_pp.sample(frac=1, random_state=seed).reset_index(drop=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WuYSHHvdtiV",
        "outputId": "59155940-3430-4532-a190-60eb56202ba9"
      },
      "source": [
        "df_pp.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(274449, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa1VKH-kd0SM",
        "outputId": "92ad67ab-76e7-4545-8289-e97fa6b1b434"
      },
      "source": [
        "# This section does the split between train/dev for the parallel corpora then saves them as separate files\n",
        "# We use 1000 dev test and the given test set.\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# Do the split between dev/train and create parallel corpora\n",
        "num_dev_patterns = 1000\n",
        "\n",
        "# Optional: lower case the corpora - this will make it easier to generalize, but without proper casing.\n",
        "if lc:  # Julia: making lowercasing optional\n",
        "    df_pp[\"source_sentence\"] = df_pp[\"source_sentence\"].str.lower()\n",
        "    df_pp[\"target_sentence\"] = df_pp[\"target_sentence\"].str.lower()\n",
        "\n",
        "#Divide df_pp into train and test sets\n",
        "\n",
        "msk = np.random.rand(len(df_pp)) < 0.98\n",
        "train = df_pp[msk]\n",
        "test_df = df_pp[~msk]\n",
        "\n",
        "# Further divide train into train and additional dev set which should be the last 1000\n",
        "dev_df = train.tail(num_dev_patterns) # Herman: Error in original\n",
        "stripped_df = train.drop(train.tail(num_dev_patterns).index)\n",
        "\n",
        "with open(\"test.\"+source_language, \"w\") as src_file, open(\"test.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in test_df.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in stripped_df.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "    \n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in dev_df.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "\n",
        "#stripped[[\"source_sentence\"]].to_csv(\"train.\"+source_language, header=False, index=False)  # Herman: Added `header=False` everywhere\n",
        "#stripped[[\"target_sentence\"]].to_csv(\"train.\"+target_language, header=False, index=False)  # Julia: Problematic handling of quotation marks.\n",
        "\n",
        "#dev[[\"source_sentence\"]].to_csv(\"dev.\"+source_language, header=False, index=False)\n",
        "#dev[[\"target_sentence\"]].to_csv(\"dev.\"+target_language, header=False, index=False)\n",
        "\n",
        "# Doublecheck the format below. There should be no extra quotation marks or weird characters.\n",
        "! head train.*\n",
        "! head dev.*\n",
        "! head test.*"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.en <==\n",
            "2 God Does Not Really Care ​ — Is It True ?\n",
            "In the following article , we will expand on aspects of discipline within the family and the congregation .\n",
            "Six branches were combined under the direction of the Mexico branch\n",
            "Eber\n",
            "We recognize maturity in a person when we see discernment , insight , wisdom , and so on .\n",
            "Paul and Silas were able to give a fine witness to the jailer and his family .\n",
            "Instead of thinking that we always know best , may we humbly listen to wise counselors who seek to help us by using the Scriptures .\n",
            "• Why is it helpful to preach to people in their mother tongue ?\n",
            "And because I am more available to help with the children , Ruth has been able to auxiliary pioneer from time to time .\n",
            "The inspired account states : “ After that God saw everything he had made and , look !\n",
            "\n",
            "==> train.sg <==\n",
            "2 Nzapa ayeke bi bê ti lo biani na e pëpe : A yeke tâ tënë la ?\n",
            "Na yâ ti article ti peko , e yeke sara tënë mingi na ndö ti fango lege na yâ ti sewa nga na congrégation .\n",
            "A bungbi afiliale omene na gbe ti filiale ti Mexique\n",
            "Héber\n",
            "E hinga dutingo biazo ti mbeni zo tongana e ba so lo fa na gigi gbungo nda ti ye , ngangu ti bango nda ti ye , ndara , na ambeni lengo nde tongaso .\n",
            "Na pekoni , Paul na Silas afa tënë na zo ti batango da ti kanga ni nga na azo ti sewa ti lo .\n",
            "Ahon ti pensé so a yeke gi atënë ti e si ayeke lakue na lege ni , zia e mä na tâ be - ti - molenge awawango ti ndara so agi ti mû maboko na e na lege ti Mbeti ti Nzapa .\n",
            "• Ngbanga ti nyen a yeke nzoni ti fa tënë na azo na yanga ti kodoro ti ala ?\n",
            "Nga , teti so mbi yeke na ngoi ti bâ lege ti amolenge , Ruth awara lege ti mû kua ti pionnier auxiliaire ngoi na ngoi .\n",
            "Tondo so a sû na gbe ti yingo vulu atene : “ Nzapa abâ ye kue so Lo sala , na bâ , a yeke nzoni mingi . ”\n",
            "==> dev.en <==\n",
            "By speaking “ consolingly to the depressed souls , ” elders truly support their spiritual brothers and sisters who may be getting weary or downhearted because of some thorn in their flesh . — Isaiah 32 : 2 ; 50 : 4 ; 1 Thessalonians 5 : 14 . 10 , 11 . How can God’s servants encourage others who are undergoing severe tests ?\n",
            "We find the evidence in the book of Genesis .\n",
            "But Christian parents need to answer this more important question , ‘ Should my children be educated ? ’\n",
            "All of us are in the same congregation .\n",
            "Jesus ’ disciples had faith in God , and as a result , they were able to endure the severe persecution they experienced .\n",
            "He taught us to use it to defend our faith\n",
            "Do not be afraid . ’ ” Note , however , that Paul’s admonition to “ speak consolingly to the depressed souls ” and to “ support the weak ” was not given to elders only .\n",
            "These are “ desirable things , ” and they are far more precious to Jehovah than all the finery that adorned Solomon’s temple .\n",
            "The way a person lives puts him on one side of the issue or the other .\n",
            "As they walked along , the mother tried to comfort her weeping daughter , who struggled to understand why this had happened .\n",
            "\n",
            "==> dev.sg <==\n",
            "Na salango tënë ti “ lungula vundu ti ala so bê ti ala anze , ” a - ancien amû biani maboko na aita ti ala ti koli na ti wali na lege ti yingo , so peut - être bê ti ala anze wala ayeke na kota vundu ndali ti mbeni kî na yâ mitele ti ala . ​ — Esaïe 32 : 2 ; 50 : 4 , NW ; 1 aThessalonicien 5 : 14 .\n",
            "Buku ti Genèse afa ni polele .\n",
            "Me a yeke ti ababâ na mama Chrétien ti kiri tënë na kota hundango tënë so : ‘ A lingbi amolenge ti mbi amanda mbeti ? ’\n",
            "E kue e yeke na yâ ti congrégation oko .\n",
            "Adisciple ti Jésus amä na bê na Nzapa , na tongana ye ti pekoni , ala wara lege la ni ti luti na gbele kota ye ti ngangu so asi na ala .\n",
            "Lo fa na e ti sala kusala na ni ti kiri tënë teti mabe ti e\n",
            "Ye oko , bâ so wango ti Paul ti “ lungula vundu ti ala so bê ti ala anze ” na ti “ sala na ala so ayeke na ngangu pëpe ” ayeke pëpe mbeni wango so a mû gi teti a - ancien .\n",
            "Azo so ayeke “ aye ti pendere , ” na ala yeke na ngele ngangu mingi na lê ti Jéhovah ahon apendere ye so aleke na temple ti Salomon .\n",
            "Lege so azo ayeke sara na ye afa wala ala mû mbage ti tâ tënë wala ala mû mbage ti mvene .\n",
            "Na lege , mama ni atara ti dë bê ti molenge ni so ayeke toto , teti lo gi li ti lo gbä na ye so asi .\n",
            "==> test.en <==\n",
            "While there are a variety of factors that can affect our feelings about ourselves , the way others view or treat us plays a large role in our sense of personal value in day - to - day life .\n",
            "He lost sight of the bigger issues .\n",
            "The opening song explains what has saved Asaph from being led astray by erroneous thinking .\n",
            "Jehovah “ felt regret ” in the sense that he altered his dealings with the Ninevites because they changed their ways .\n",
            "He also preached to an Ethiopian official who responded to the message about Christ . ​ — Acts 6 : 1 - 5 ; 8 : 5 - 13 , 26 - 40 ; 21 : 8 , 9 .\n",
            "Did Adam and Eve’s choice eventually enhance their freedom in any way ?\n",
            "Our faith is renewed , and our hope confirmed . ​ — Ephesians 3 : 16 - 19 .\n",
            "Brothers being led away after their trial\n",
            "What is needed to receive “ a good reward ” in marriage ?\n",
            "This begins with dedication and baptism .\n",
            "\n",
            "==> test.sg <==\n",
            "Atâa so aye nde nde alingbi ti ndu lege so e bâ na tere ti e , mingi ni a yeke sarango ye ti azo na mbage ti e si afa na e nengo ti e na yâ gigi ti lâ na lâ .\n",
            "Lo girisa kota tënë so a lingbi a leke ni .\n",
            "Psaume 73 ni afa ye so abata Asaph si sioni bibe ague na lo yongoro na Nzapa pëpe .\n",
            "Na lege wa Jéhovah “ ake sioye ” so lo ye ti sala ? Na lege so lo gbian ye so lo leke ti sala na azo ti Ninive teti so ala gbian lege ti ala .\n",
            "Nga lo fa tënë na mbeni kota zo ti Ethiopie so ayeda na tënë ti Christ . ​ — Kusala 6 : 1 - 5 ; 8 : 5 - 13 , 26 - 40 ; 21 : 8 , 9 .\n",
            "Eskê ye so Adam na Ève asoro asara si ala wara liberté mingi tongana ti so Satan atene ?\n",
            "Mabe ti e akiri awara ngangu , na beku ti e akpengba biani . ​ — aEphésien 3 : 16 - 19 .\n",
            "A mû aita a yeke hon na ala na peko ti so afâ ngbanga ti ala\n",
            "A lingbi mbeni koli na wali ti lo asara nyen ti wara ‘ futa ti nzoni ’ ?\n",
            "Ye so ato nda ni na mungo tere ti e so e mû na lo nga na batême .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqljY_y4d6fz"
      },
      "source": [
        "stripped_df.to_csv('train.csv')\n",
        "!cp train.csv drive/My\\ Drive/masakhane/"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKzlNQq4eJLa"
      },
      "source": [
        "test_df.to_csv('test.csv')\n",
        "!cp test.csv drive/My\\ Drive/masakhane/"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsrCSCTveMvL"
      },
      "source": [
        "\n",
        "dev_df.to_csv('dev.csv')\n",
        "!cp dev.csv drive/My\\ Drive/masakhane/"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuNMTzI8eO8T",
        "outputId": "23380cf7-1028-4a7b-d2af-b7151a2580de"
      },
      "source": [
        "\n",
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 2724 (delta 4), reused 4 (delta 1), pack-reused 2711\u001b[K\n",
            "Receiving objects: 100% (2724/2724), 2.80 MiB | 14.06 MiB/s, done.\n",
            "Resolving deltas: 100% (1865/1865), done.\n",
            "Processing /content/joeynmt\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from joeynmt==1.0) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from joeynmt==1.0) (7.0.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from joeynmt==1.0) (1.18.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from joeynmt==1.0) (50.3.2)\n",
            "Collecting torch==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/53/914885a93a44b96c0dd1c36f36ff10afe341f091230aad68f7228d61db1e/torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |███████████████████████████████▉| 745.8MB 1.4MB/s eta 0:00:03"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hspdVEUYl3nG"
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw6PKfOXmkgl"
      },
      "source": [
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-kfuDz1nQqc"
      },
      "source": [
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXCHjpWleRUJ"
      },
      "source": [
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path \"$gdrive_path/vocab.txt\"\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Hausa Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "#! tail -n 10 joeynmt/data/$src$tgt/vocab.txt\n",
        "! tail -n 10 \"$gdrive_path/vocab.txt\" #lmm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX8hBqs1gE11"
      },
      "source": [
        "def get_last_checkpoint(directory):\n",
        "  last_checkpoint = ''\n",
        "  try:\n",
        "    for filename in os.listdir(directory):\n",
        "      if 'best' in filename and filename.endswith(\".ckpt\"):\n",
        "        return filename\n",
        "      if not 'best' in filename and filename.endswith(\".ckpt\"):\n",
        "          if not last_checkpoint or int(filename.split('.')[0]) > int(last_checkpoint.split('.')[0]):\n",
        "            last_checkpoint = filename\n",
        "  except FileNotFoundError as e:\n",
        "    print('Error Occur ', e)\n",
        "  return last_checkpoint\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L36mahhZrgjK"
      },
      "source": [
        "# Copy the created models from the temporary storage to main storage on google drive for persistant storage \n",
        "# the content of te folder will be overwrite when you start trainin\n",
        "!cp -r \"/content/drive/My Drive/masakhane/model-temp/\"* \"$gdrive_path/models/${src}${tgt}_transformer/\"\n",
        "last_checkpoint = get_last_checkpoint(models_path)\n",
        "print('Last checkpoint :',last_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-DVaT5Ari7l"
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "\n",
        "# very important !!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "# uncomment line load_model if you want to use a checkpoint ..\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"{gdrive_path}/train.bpe\"\n",
        "    dev:   \"{gdrive_path}/dev.bpe\"\n",
        "    test:  \"{gdrive_path}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"{gdrive_path}/vocab.txt\"\n",
        "    trg_vocab: \"{gdrive_path}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/{last_checkpoint}\" # uncommented to load a pre-trained model from last checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"{model_temp_dir}\"\n",
        "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, last_checkpoint=last_checkpoint, model_temp_dir=model_temp_dir, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cbS3657r1hL"
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfLChg3a4OP4"
      },
      "source": [
        "# Copy the created models from the temporary storage to main storage on google drive for persistant storage \n",
        "!cp -r \"/content/drive/My Drive/masakhane/model-temp/\"* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG2g840h4OCW"
      },
      "source": [
        "# # Copy the created models from the notebook storage to google drive for persistant storage \n",
        "# !cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8g8kypb4Yy5"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mWfVMh5r90x"
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPielAuW4B8Y"
      },
      "source": [
        "# python3 plot_validation.py \"$gdrive_path/models/${src}${tgt}_transformer/\" --plot_values bleu PPL --output_path my_plot.pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JGhfttk4FXD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}